{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [VGG](https://arxiv.org/pdf/1409.1556.pdf)\n",
        "\n",
        "Implement VGG16, for that write specific `nn.Module`, `VGGBlock` implementing block of VGG."
      ],
      "metadata": {
        "id": "od-IHzGCC2sr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITvQtQzDC1MN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self, in_channels, max_features=1000):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.max_features = max_features\n",
        "\n",
        "        self.vggb1 = self.VGGBlock(2, self.in_channels, 64)\n",
        "        self.vggb2 = self.VGGBlock(2, 64, 128)\n",
        "        self.vggb3 = self.VGGBlock(3, 128, 256)\n",
        "        self.vggb4 = self.VGGBlock(3, 256, 512)\n",
        "        self.vggb5 = self.VGGBlock(3, 512, 512)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=1000, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vggb1(x)\n",
        "        x = self.vggb2(x)\n",
        "        x = self.vggb3(x)\n",
        "        x = self.vggb4(x)\n",
        "        x = self.vggb5(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return nn.Softmax(x)\n",
        "\n",
        "    def VGGBlock(self, convs_num, in_channels, out_channels):\n",
        "        res = []\n",
        "        res.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1))\n",
        "        res.append(nn.ReLU())\n",
        "        for i in range(convs_num - 1):\n",
        "            res.append(\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1)\n",
        "            )\n",
        "            res.append(nn.ReLU())\n",
        "        res.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
        "        return nn.Sequential(\n",
        "            *res\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf)\n",
        "\n",
        "## Inception module\n",
        "\n",
        "Write specific `nn.Module` for Inception module."
      ],
      "metadata": {
        "id": "Gi43r1dPDRp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class Inception(nn.Module):\n",
        "    # `c1`--`c4` are the number of output channels for each branch\n",
        "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
        "        super(Inception, self).__init__(**kwargs)\n",
        "        # Branch 1\n",
        "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
        "        # Branch 2\n",
        "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
        "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
        "        # Branch 3\n",
        "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
        "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
        "        # Branch 4\n",
        "        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = F.relu(self.b1_1(x))\n",
        "        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n",
        "        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n",
        "        b4 = F.relu(self.b4_2(self.b4_1(x)))\n",
        "        return torch.cat((b1, b2, b3, b4), dim=1)"
      ],
      "metadata": {
        "id": "L91nFCXhDhxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stem network\n",
        "\n",
        "Write down, why do we need a Stem network."
      ],
      "metadata": {
        "id": "Pot5itHXDisN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Write your answer here.*\n",
        "\n",
        "It consists of three convolutional layers (the first with a large filter) with two unifying layers and is located at the very beginning of the architecture. The purpose of this subnetwork is to quickly and strongly reduce the spatial dimensions (compress the image before parallel processing) in order to minimize the number of elements in the layers."
      ],
      "metadata": {
        "id": "nhADh2IVDojD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [ResNet](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "Implement ResNet-18, for that write specific `nn.Module`, `ResNetBlock` implementing block of ResNet."
      ],
      "metadata": {
        "id": "uD7dsr2yDuGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    \"\"\"The Residual block of ResNet.\"\"\"\n",
        "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
        "                                   stride=strides)\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "\n",
        "    def forward(self, x):\n",
        "        Y = self.conv1(x)\n",
        "        Y = self.bn1(Y)\n",
        "        Y = F.relu(Y)\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            x = self.conv3(x)\n",
        "        Y += x\n",
        "        return F.relu(Y)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.net = nn.Sequential(self.b1())\n",
        "        for i, b in enumerate(arch):\n",
        "            self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
        "        self.net.add_module('last', nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.LazyLinear(num_classes)))\n",
        "\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        \n",
        "    def block(self, num_residuals, num_channels, first_block=False):\n",
        "        blk = []\n",
        "        for i in range(num_residuals):\n",
        "            if i == 0 and not first_block:\n",
        "                blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
        "            else:\n",
        "                blk.append(Residual(num_channels))\n",
        "        return nn.Sequential(*blk)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class ResNet18(ResNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
        "                       lr, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "ResNet18()"
      ],
      "metadata": {
        "id": "FJxA2J7OEPxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51f6c6a-97e7-4091-a582-6120a3aa2f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18(\n",
              "  (net): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): LazyConv2d(0, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
              "      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (b2): Sequential(\n",
              "      (0): Residual(\n",
              "        (conv1): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (conv1): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (b3): Sequential(\n",
              "      (0): Residual(\n",
              "        (conv1): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv3): LazyConv2d(0, 128, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (conv1): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (b4): Sequential(\n",
              "      (0): Residual(\n",
              "        (conv1): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv3): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (conv1): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (b5): Sequential(\n",
              "      (0): Residual(\n",
              "        (conv1): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv3): LazyConv2d(0, 512, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (conv1): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (bn2): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (last): Sequential(\n",
              "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      (1): Flatten(start_dim=1, end_dim=-1)\n",
              "      (2): LazyLinear(in_features=0, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Слегка странно, что модель строится из init, а не из forward метода, но там есть всё необходимое (в блоке)."
      ],
      "metadata": {
        "id": "d1q02O2gB8Fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [ResNeXt](https://arxiv.org/pdf/1611.05431.pdf)\n",
        "\n",
        "Write specific `nn.Module`, `ResNeXtBlock` implementing block of ResNeXt."
      ],
      "metadata": {
        "id": "cAkVh_wLES6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNeXtBlock(nn.Module):\n",
        "    \"\"\"The ResNeXt block.\"\"\"\n",
        "    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\n",
        "                 strides=1):\n",
        "        super().__init__()\n",
        "        bot_channels = int(round(num_channels * bot_mul))\n",
        "        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,\n",
        "                                   stride=strides, padding=1,\n",
        "                                   groups=bot_channels//groups)\n",
        "        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "        self.bn3 = nn.LazyBatchNorm2d()\n",
        "        if use_1x1conv:\n",
        "            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "            self.bn4 = nn.LazyBatchNorm2d()\n",
        "        else:\n",
        "            self.conv4 = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
        "        Y = self.bn3(self.conv3(Y))\n",
        "        if self.conv4:\n",
        "            X = self.bn4(self.conv4(X))\n",
        "        return F.relu(Y + X)"
      ],
      "metadata": {
        "id": "6FN7E2GADnTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [SENet](https://arxiv.org/pdf/1709.01507.pdf)\n",
        "\n",
        "Write specific `nn.Module`, `SEBlock` implementing block of SENet."
      ],
      "metadata": {
        "id": "ebgJY9YcEw2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SE_Block(nn.Module):\n",
        "    def __init__(self, c, r=16):\n",
        "        super().__init__()\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(c, c // r, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(c // r, c, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, c, _, _ = x.shape\n",
        "        y = self.squeeze(x).view(bs, c)\n",
        "        y = self.excitation(y).view(bs, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ],
      "metadata": {
        "id": "BfddhrMkE5un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Neural Architecture Search](https://arxiv.org/pdf/1611.01578.pdf)\n",
        "\n",
        "For the neural network of your assignment 2, write down the parametrization of the network you would use for the NAS."
      ],
      "metadata": {
        "id": "px-Os_viE-aM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Write your answer here.*\n",
        "\n",
        "Для своей сети я смогу подбирать количество слоёв, выбирать функцию активации, количество нейронов в слоях. В принципе всё, у меня не свёрточная нейронная сеть, так что вот."
      ],
      "metadata": {
        "id": "sojINj1YFeMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STATE_SIZE = 9\n",
        "ACTIONS_SIZE = 4\n",
        "HIDDEN_LAYER_SIZE = 2 ** STATE_SIZE\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input = nn.Linear(STATE_SIZE, HIDDEN_LAYER_SIZE)\n",
        "        self.hidden_1 = nn.Linear(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE // 2)\n",
        "        self.hidden_2 = nn.Linear(HIDDEN_LAYER_SIZE // 2, HIDDEN_LAYER_SIZE // 2)\n",
        "        self.output = nn.Linear(HIDDEN_LAYER_SIZE // 2, ACTIONS_SIZE)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input(x)\n",
        "        x = self.relu(self.hidden_1(x))\n",
        "        x = self.relu(self.hidden_2(x))\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return self.softmax(x)"
      ],
      "metadata": {
        "id": "VuLyxepjE6zL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}